The Universal Approximation Theorem states that a feedforward neural network can theoretically approximate continuous functions on compact subsets of $\mathbb{R}^n$ under mild assumptions on the activation function. This theorem is a cornerstone in the field of neural networks, as it provides theoretical justification for their expressive power.

This repository breaks down the theorem, its assumptions, and the steps Cybenko skipped over in an easy-to-understand manner.
